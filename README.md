# 🧩 Reproducibility Package for [Text2Stories]

This repository provides the data, experimental setup, and evaluation scripts used in the paper:

> **[Text2Stories: Evaluating the Alignment Between Stakeholder Interviews and Generated User Stories]**  

---

## 📁 Repository Structure
```
├── data/
│   ├── public_interview/          # Publicly released interview dataset
│   ├── public_system_desc/         # Publicly released system description dataset
│   └── generated_US/               # User stories generated by LLMs for 15 private datasets
│
├── eval/
│   ├── ablation_chunking_alignment_quality/     # Ablation on pairwise vs full-context alignment
│   ├── ablation_chunking_blocking_efficiency/   # Ablation on different chunking strategies for blocking efficiency
│   ├── ablation_metric_robustness/              # Robustness of the metric for detecting mismatches
│   ├── ablation_threshold_calibration/          # Effect of threshold calibration for cross-encoders
│   ├── pairwise_alignment_quality/              # Experiments for pairwise alignment quality
│   ├── text2stories_metric/                     # Evaluation of correctness and completeness
│   └── blocking_operator/                       # Evaluation of the blocking operator's efficiency
│
├── train/                          # Training scripts for embedding models used in the blocking operator
└── README.md
```


---

## 📦 Data

The `data/` folder contains:

- `public_interview/`: Publicly released dataset used in our experiments.
- `public_system_desc/` : Publicly released dataset used in our experiments.
- `generated_US/`: Synthetic user stories generated using a large language model (LLM) for **15 non-public datasets**. The original datasets are not shareable due to privacy restrictions.

---

## 🔬 Experiments

The `eval/` folder replicates all experiments from **Section 6** of the paper, including:

1. **Ablation Studies**:
   - Four ablation experiments to test the contribution of individual components.

2. **Pairwise Alignment Quality**:
   - Evaluates how well our method aligns user stories with elicitation interview chunks.

3. **Correctness and Completeness**:
   - Measures how accurately the generated stories reflect the elicitation interview.

4. **Efficiency of the Blocking Operator**:
   - Benchmarks the runtime and scaling behavior of our blocking mechanism.

---

## ▶️ Requirements
Requirements are in pyproject.toml (not all requirements are strictly needed for running these scripts)
